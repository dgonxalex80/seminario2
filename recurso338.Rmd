---
title: "**Modelo de regresión y curvas de calibración**"
author: "Seminario"
date: " "
output: 
  html_document:
    code_folding: hide
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, comment = NA)
library(ggplot2)

gen.corr.data<- function(rho,n){
  x <- rnorm(n)
  z <- rnorm(n)
  y<- rho*x + sqrt(1-rho^2)*z
    result <-cbind(y,x)
  return(result)
}


# Crear datos de ejemplo
set.seed(123)
x <- 1:10
y <- 2 * x + rnorm(10)

# Realizar una regresión lineal simple
modelo <- lm(y ~ x)

# Crear un dataframe con los datos y las predicciones del modelo
df <- data.frame(x = x, y = y, yhat = predict(modelo))

```

<br/><br/>

# **Modelo de Regresión lineal**

<br/>

## **Introducción**

La primera forma de regresión lineal documentada fue el método de los mínimos cuadrados que fue publicada por Legendre en 1805, Gauss publicó un trabajo en donde desarrollaba de manera más profunda el método de los mínimos cuadrados y en dónde se incluía una versión del teorema de Gauss-Márkov. El término regresión se utilizó por primera vez en el estudio de variables antropométricas de Francis Galton: al comparar la estatura de padres e hijos, donde resultó que los hijos cuyos padres tenían una estatura muy superior al valor medio, tendían a igualarse a este, mientras que aquellos cuyos padres eran muy bajos tendían a reducir su diferencia respecto a la estatura media; es decir, "regresaban" al promedio. La constatación empírica de esta propiedad se vio reforzada más tarde con la justificación teórica de ese fenómeno.*

<br/>

El término lineal se emplea para distinguirlo del resto de técnicas de regresión, que emplean modelos basados en cualquier clase de función matemática. Los modelos lineales son una explicación simplificada de la realidad, mucho más ágiles y con un soporte teórico mucho más extenso por parte de la matemática y la estadística.

<br/>

El modelo de regresión lineal simple tiene la siguiente forma:

$$y_{i} = \beta_{0} + \beta_{1} x_{i} + u_{i} \hspace{1cm} \text{para} \hspace{.5cm}i= 1,2,3,4, \cdots , n$$
<br/>

Donde :

$Y$ : variable dependiente o respuesta <br/>
$X$ : varialbe independiente o regresora <br/>
$\beta_{0}$ : intercepto  <br/>
$\beta_{1}$ : pendiente  <br/>
$u$ : error

<br/><br/>

```{r}
# # Crear datos de ejemplo
# set.seed(123)
# x <- 1:10
# y <- 2 * x + rnorm(10)
# 
# # Realizar una regresión lineal simple
# modelo <- lm(y ~ x)
# 
# # Crear un dataframe con los datos y las predicciones del modelo
# df <- data.frame(x = x, y = y, Prediccion = predict(modelo))

# Crear un gráfico de dispersión y la línea de regresión
grafico <- ggplot(data = df, aes(x = x, y = y)) +
    geom_line(aes(y = yhat), color = "#FF7F00") +  # Línea de regresión
    geom_point() +  # Gráfico de dispersión
    labs(title = "Regresión Lineal Simple",
       x = "Variable independiente (x)",
       y = "Variable dependiente (y)")

grafico
```

<br/><br/>

## **Correlación**

<br/>

El grado de ajuste de los puntos con la recta, se mide  a través de un coeficiente que presenta valores entre -1 y 1. 

Los siguientes gráficos iluistran las relaciones simuladas para correlaciones de 1, 0.99, 0.95, 0.90, 0.75, 0.50, 0.30 y 0.00.

<br/><br/>

```{r, fig.width=8, fig.height=4}

par(mfrow = c(1, 2),
    mar = c(5, 4, 4, 2) + 0.1,
                cex.lab=.7,  # tamaño de etiqueta ejes
                cex.axis=.7, # tamaño escalas de los ejes 
                cex.main=1, # tamaño del titulo
                cex.sub=.5)

muestra<-gen.corr.data(1,20)
x=muestra[,2]+100
y=muestra[,1]+100
plot(x,y, pch=19,main="Correlación Rho= 1.0", las=1)
grid()

muestra<-gen.corr.data(0.99,20)
x=muestra[,2]+100
y=muestra[,1]+100
plot(x,y, pch=19,main="Correlación Rho= 0.99", las=1)
grid()

muestra<-gen.corr.data(0.95,20)
x=muestra[,2]+100
y=muestra[,1]+100
plot(x,y, pch=19,main="Correlación Rho= 0.95", las=1)
grid()

muestra<-gen.corr.data(0.90,20)
x=muestra[,2]+100
y=muestra[,1]+100
plot(x,y, pch=19,main="Correlación Rho=-0.90", las=1)
grid()

muestra<-gen.corr.data(0.75,20)
x=muestra[,2]+100
y=muestra[,1]+100
plot(x,y, pch=19,main="Correlación Rho=0.75", las=1)
grid()

muestra<-gen.corr.data(0.50,20)
x=muestra[,2]+100
y=muestra[,1]+100
plot(x,y, pch=19,main="Correlación Rho= 0.50", las=1)
grid()


muestra<-gen.corr.data(0.30,20)
x=muestra[,2]+100
y=muestra[,1]+100
plot(x,y, pch=19,main="Correlación Rho= 0.30", las=1)
grid()


muestra<-gen.corr.data(0.10,20)
x=muestra[,2]+100
y=muestra[,1]+100
plot(x,y, pch=19,main="Correlación Rho=0.10", las=1)
grid()


muestra<-gen.corr.data(0,20)
x=muestra[,2]+100
y=muestra[,1]+100
plot(x,y, pch=19,main="Correlación Rho=0.00", las=1)
grid()

```	


<br/><br/>

Tambien puede ser una relación negativa lineal


```{r, fig.width=8, fig.height=4}

par(mfrow = c(1, 2),
    mar = c(5, 4, 4, 2) + 0.1,
                cex.lab=.5,  # tamaño de etiqueta ejes
                cex.axis=.5, # tamaño escalas de los ejes 
                cex.main=1, # tamaño del titulo
                cex.sub=.5)

muestra<-gen.corr.data(-1,20)
x=muestra[,2]+100
y=muestra[,1]+100
plot(x,y, pch=19,main="Correlación Rho= -1.00", las=1)
grid()

muestra<-gen.corr.data(-0.95,20)
x=muestra[,2]+100
y=muestra[,1]+100
plot(x,y, pch=19,main="Correlación Rho= -0.95", las=1)
grid()
```

<br/>

Valores positivos, significa que a medida que la variable  X aumenta, la variable Y también tiende a aumentar. Un valor cercano a 1 indica una fuerte correlación positiva.

Valores de correlación negativos, significa que a medida que la variable X aumenta, la variable Y tiende a disminuir. Un valor cercano a -1 indica una fuerte correlación.

<br/><br/>

### **Nota**

El coeficiente de correlación empleado para el caso de dos variables numéricas corresponde al coeficiente de correlación de Pearson. Este coeficiente mide especificamente relaciones lineales. En caso de que la relación no sea una linea recta y por ejemplo sea una relación cuadrática, el coeficiente indicará un valor bajo.


El que existea un alto grado de correlación no implica caudalida, es decir que si dos valibles tiene un indicador alto de correlación, esto no implica que una variable cause a la otra. 


<br/><br/>

# **Métodos de Mínimos Cuadrados Ordinarios**

El método Mínimos Cuadrados Ordinarios (**MCO**) se basa en la minimizar la suma de cuadrados de los errores ($u$), a través de la información contenida en una muestra 

De la ecuación del modelo estimado  $\widehat{y_{i}} = \widehat{\beta_{0}} + \widehat{\beta_{1}} +  \widehat{u}$ se pueden despejar los residuales . Como los errores son variables no observables, el método hace uso de sus respectivos estimadores llamados residuales:

$$\begin{equation*}
	\widehat{u}_{i} = (y - \widehat{y_{i}}) = \big(y - \widehat{\beta_{0}} + \widehat{\beta_{1}} x_{i} \big)
	\end{equation*}$$
	
<br/><br/>

El método encuentra el valor de los parámetros $\beta_{0}$ y $\beta_{1}$  que minimzan  la suma de cuadrados de los erroes $SCRes$


$$SCRes=\sum_{i=1}^{n} \widehat{u_{i}}^{2} = \big(y - \widehat{\beta_{0}} + \widehat{\beta_{1}} \hspace{.2cm}x_{i} \big)^{2} $$

Para ello deriva la función anterior con respecto a $\beta_{0}$ y $\beta_{1}$


$$\dfrac{\partial SCRes}{\partial\beta_{0}} = -2 \sum_{i=1}^{n} \big(y - \widehat{\beta_{0}} + \widehat{\beta_{1}} \hspace{.2cm}x_{i} \big)  = 0$$
$$\dfrac{\partial SCRes}{\partial\beta_{1}}  = -2 \sum_{i=1}^{n} \big(y - \widehat{\beta_{0}} + \widehat{\beta_{1}} \hspace{.2cm}x_{i} \big)\hspace{.2cm} x_{i}  = 0$$


Estas dos igualdades constituten las ecuaciones normales de las que se da solución al sistema $2 \times 2$.

<br/><br/>

**Solución**:


$$b_1 = \dfrac{n \displaystyle\sum_{i=1}^{n} x_{i}y_{i} - \Bigg(\displaystyle\sum_{i=1}^{n} x_{i} \displaystyle\sum_{i=1}^{n} y_{i}\Bigg) }{n \displaystyle\sum_{i=1}^{n} x_{i}^{2} - \Bigg(\displaystyle\sum_{i=1}^{n} x_{i}\Bigg)^{2}}  $$


$$b_{0} = \bar{y} - b_{1} \hspace {.1cm} \bar{x} $$
<br/><br/>

Estos los coeficientes estimados del intercepto y de la pendiente de la recta $\widehat{y} = b_{0} + b_{1} x$


Como los vales de $b_{0}$ y $b_{1}$ son variables aleatorias, dado que sus valores cambian al cambiar la muestra que los origina,  poseen entonce una función de distribución, un valor esperado o media y una varianza.

Los supuestos asociados al modelo con fines de inferencia estadistica se pueden resumir en :

* Los errores tienen media cero
* Los errores tienen varianza constante
* Los errores son normales
* Los errores son independientes


La verificación de estos supuesto se puede realizar mediante la realización de pruebas de hipotesis complementadas con gráficos


```{r}
modelo1 = lm(y ~ x)

summary(modelo1)
```

Ecuación estimada del modelo por el método de mínimos cuadrados ordinarios es: 


$$ \widehat{y_{i}} = 0.5255 + 1.9180 \hspace{.2cm} x_{i} $$
La salida tambien contiene además de los valores obtenidos:

* Pruebas de hipotesis individuales
  * $Ho : \beta_{0} = 0$  vs $Ha: \beta_{0} \neq 0$
  * $Ho : \beta_{1} = 0$  vs $Ha: \beta_{1} \neq 0$

En este caso no se rechaza la hipótesis : $Ho : \beta_{0} = 0$, se asume que es verdadera por lo que se puede asumir que el intercepto es cero.

En cuanto a la pendiente el valor-p = 0.0000, indica que se rechaza la hipótesis : $Ho : \beta_{1} = 0$, se acepta que $\beta_{1} \neq 0$, por tal razón existe un modelo lineal que relaciona a las variables.

* $R^{2}$ (R-squarred) : que indica el porcentaje de la variable Y explicada por el modelo. En este caso el resultado indica que el modelo explica en un 97.55% la variación o comportamiento de la variable Y.


No te que al exigir al modelo que no tenga intercepto, la pendiente cambia

```{r}
modelo2 = lm(y ~ x -1)

summary(modelo2)
```


Se recomienda que aunque la prueba de hipótesis correspondiente al intercepto de no significativa (se asuma que es cero), se conserve el modelo con intercepto, pues de lo contrario se puede generar sesgos sobre la estimación de la pendiente.

<br/><br/>



## **Curvas de calibración**


Una curva de calibración es una representación gráfica de la relación entre la respuesta instrumental y la concentración de un analito en una serie de soluciones estándar. Las curvas de calibración se utilizan en química analítica para determinar la concentración de un analito en una muestra desconocida.

El principio de funcionamiento de las curvas de calibración se basa en la relación proporcional entre la respuesta instrumental y la concentración del analito. La respuesta instrumental es una medida de la cantidad de analito que se ha detectado por el instrumento. La concentración del analito es la cantidad de analito que se encuentra en una unidad de volumen de la muestra.

Existen dos tipos principales de curvas de calibración:

<br/><br/>

### **Curvas de calibración lineales**: 

Estas curvas son las más comunes. La respuesta instrumental es directamente proporcional a la concentración del analito.

<br/><br/>


### **Curvas de calibración no lineales**: 

Estas curvas no son directamente proporcionales a la concentración del analito. En este caso, la curva de calibración se ajusta a una ecuación matemática para determinar la concentración del analito.

<br/><br/>


### **Procedimiento**

Se toma una serie de materiales - normalmente al menos tres o cuatro - de los que se conoce la concentración del analito.

Estos patrones de calibración son medidos en el instrumento analítico bajo las mismas condiciones para las empleadas para el material de ensayo


Con los resulados obtenidos se construye la gráfica de calibrado con el fin de obtener la concentración de analito por interpolación. 

Este procedimiento plantea una serie de interrogantes como:

* Es lineal o no la línea o curva de calibración?
* Cuál es la mejor recta o curva que pasa por estos puntos?
* Suponiendo que es lineal, cúales son los errores y límites de confianza para la pendiente y el intercepto?
* Cuáles son los errors y límites de confianza de la concentración determinada?
* Cuál la menor concentración de analito que puede detectarser con un nivel de confianza?

<br/><br/>

### **Ejemplo**

Se miden 4 nivels de concentración (40, 100, 150, 300), cada uno de ellos es replicado 5 veces, obteniendo los siguientes valores:


```{r}
library(ggplot2)

# Crear datos de absorbancia y concentración
absorbancia <- c(0.125, 0.135, 0.120, 0.130, 0.128, 0.315, 0.325, 0.310, 0.320, 0.312, 0.472, 0.481, 0.468, 0.475, 0.470, 0.948, 0.965, 0.940, 0.955, 0.952)
concentracion <- c(40, 40, 40, 40, 40, 100, 100, 100, 100, 100, 150, 150, 150, 150, 150, 300, 300, 300, 300, 300)

tabla = matrix(absorbancia,  ncol = 5, byrow = TRUE )
rownames(tabla) = c(40, 100, 150,300)
colnames(tabla) = c("R1", "R2", "R3", "R4", "R5")
tabla
```

<br/><br/>


Estimación de la recta por MCO

```{r}
# Crear un dataframe con los datos
datos <- data.frame(Concentracion = concentracion, Absorbancia = absorbancia)

 # Crear un dataframe con los datos
datos <- data.frame(Concentracion = concentracion, Absorbancia = absorbancia)

modelo = lm(absorbancia ~ concentracion, data = datos)

summary(modelo)
```
<br/><br/>


### **Curva de calibración**


```{r}
# Crear la gráfica con ggplot2
ggplot(datos, aes(x = Concentracion, y = Absorbancia)) +
  geom_point(size = 1, color = "blue") +  # Tamaño de punto pequeño
  geom_smooth(method = "lm", color = "red") +
  labs(x = "Concentración (mg/100 ml)", y = "Absorbancia") +
  theme_minimal()

```

<br/><br/>


Estimación de la concentración para una abserbancia de 0.200 se requiere estimar el nivel de concentración correspondiente:


```{r}
beta = coef(modelo)
new_y = 0.200

xhat <- (new_y - beta[1]) / beta[2]
names(xhat) = c("concentración") 
xhat
```

<br/><br/>

## **Curvas no lineales**


$$y = a \hspace{.2cm}exp\{b x\}$$

```{r}

library(ggplot2)
library(nls2)

# Datos de concentración conocida y propiedad medida
concentracion <- c(1, 2, 3, 4, 5)
propiedad_medida <- c(10, 20, 30, 40, 50)

# Realizar un ajuste no lineal a una función exponencial
ajuste_no_lineal <- nls(propiedad_medida ~ a * exp(b * concentracion), start = list(a = 1, b = 1))

# Obtener los parámetros ajustados
parametros <- coef(ajuste_no_lineal)

# Prueba de hipótesis para los parámetros
summary(ajuste_no_lineal)

# Intervalos de confianza para los parámetros
confint(ajuste_no_lineal)

# Gráfico de datos y curva de calibración
df <- data.frame(Concentracion = concentracion, Medida = propiedad_medida)
df$Curva_Calibracion <- parametros["a"] * exp(parametros["b"] * df$Concentracion)

grafico <- ggplot(data = df, aes(x = Concentracion, y = Medida)) +
  geom_point() +  # Puntos de datos
  geom_line(aes(x = Concentracion, y = Curva_Calibracion), color = "red") +  # Curva de calibración
  labs(title = "Curva de Calibración (Ajuste No Lineal)",
       x = "Concentración",
       y = "Propiedad Medida")

print(grafico)

# Validación cruzada simple
set.seed(123)  # Establece una semilla para la reproducibilidad
indices <- sample(1:5)  # Genera índices aleatorios para división de datos
datos_entrenamiento <- df[indices != 5, ]
datos_prueba <- df[indices == 5, ]

ajuste_validacion <- nls(Medida ~ a * exp(b * Concentracion), 
                         data = datos_entrenamiento, 
                         start = list(a = 1, b = 1))

parametros_validacion <- coef(ajuste_validacion)

# Predicción en datos de prueba
predicciones <- predict(ajuste_validacion, newdata = datos_prueba)

# Evaluación de la capacidad de predicción
error_cuadratico_medio <- sqrt(mean((datos_prueba$Medida - predicciones)^2))
cat("Error cuadrático medio en datos de prueba:", error_cuadratico_medio, "\n")

```

<br/><br/>

### **Transformación log-log**


```{r}
data2 = data.frame(
           lmedida = log(df$Medida),
           lconcentracion = log(df$Concentracion))

modelo3 = lm(lmedida ~ lconcentracion, data = data2)
summary(modelo3)

plot(data2$lmedida, data2$lconcentracion )
abline(modelo3)

```





